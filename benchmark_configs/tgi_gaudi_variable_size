#!/usr/bin/env bash
# set -ex

export PORT=80

ranges=("32")
# max_batch_total_tokens_vals=("8700" "7500" "8100" "8100")

random_prompt_lens_mean=$1
response_len=$2
response_range=$3
bs=$4
distribution=${5:-"capped_exponential"}
if [ "$distribution" == "uniform" ]; then
    max_batch_total_tokens=$(( (random_prompt_lens_mean + response_len + response_range) * bs ))
else
    max_batch_total_tokens=$(( (random_prompt_lens_mean + response_len) * bs ))
fi

echo ${max_batch_total_tokens}

model_name_dir=/ssd/hf_models/models--meta-llama--Llama-2-7b-chat-hf/snapshots/f5db02db724555f92da89c216ac04704f23d4590/
results_home=$PWD/result
mkdir -p ${results_home}
max_input=$random_prompt_lens_mean
if [ "$distribution" == "uniform" ]; then
    max_total=$(( random_prompt_lens_mean + response_len + response_range))
else
    max_total=$(( random_prompt_lens_mean + response_len))
fi
function start_model_server {
    local max_batch_total_tokens=$1

    model_name=$model_name_dir \
    max_batch_total_tokens=$max_batch_total_tokens \
    PORT=$PORT \
    max_input=$max_input \
    max_total=$max_total \
    ../launch_scripts/launch_tgi_gaudi \
 &
    
    while [ "$(curl -s http://localhost:${PORT}/info | grep $model_name_dir | wc -l)" -eq 0 ]; do
        echo 'Waiting for server to be ready'
        sleep 10
    done
    echo "model server started on port $PORT, max_batch_total_tokens $max_batch_total_tokens"
}

function kill_model_server {
    echo 'killing model server'
    ps aux | grep 'text-generation-launcher' | awk '{print $2}' | xargs kill -9
    ps aux | grep 'text-generation-router' | awk '{print $2}' | xargs kill -9
    ps aux | grep 'text-generation-server' | awk '{print $2}' | xargs kill -9
    wait
}

trap kill_model_server EXIT

# Catch OOMs early.
# for i in ${!ranges[@]}; do
#     range=${ranges[$i]}
#     max_batch_total_tokens=${max_batch_total_tokens_vals[$i]}
    

#     echo "range $range max_batch_total_tokens $max_batch_total_tokens"

#     start_model_server $max_batch_total_tokens

#     pushd ..
#         ./benchmark_throughput.py \
#             --port $PORT \
#             --backend HfTextGenerationInference \
#             --random_prompt_lens_mean 512 \
#             --random_prompt_lens_range 0 \
#             --random_prompt_count 10 \
#             --gen_random_prompts \
#             --fixed_max_tokens $range
#     popd

#     kill_model_server
# done

# Run real test
# for i in ${!ranges[@]}; do
#     range=${ranges[$i]}
#     max_batch_total_tokens=${max_batch_total_tokens_vals[$i]}
    
#     echo "range $range max_batch_total_tokens $max_batch_total_tokens"

#     start_model_server $max_batch_total_tokens

#     pushd ..
#         ./benchmark_throughput.py \
#             --port $PORT \
#             --backend HfTextGenerationInference \
#             --random_prompt_lens_mean 512 \
#             --random_prompt_lens_range 0 \
#             --random_prompt_count 10 \
#             --gen_random_prompts \
#             --variable_response_lens_mean 128 \
#             --variable_response_lens_range $range \
#             --variable_response_lens_distribution capped_exponential \
#             --allow_variable_generation_length \
#             --results_filename ${results_home}/tgi_gaudi_range_${range}_$(date '+%Y-%m-%d_%H-%M-%S').log
#     popd

#     kill_model_server
# done

# Run real test
for i in ${!ranges[@]}; do
    range=${response_range}
    
    echo "range $range max_batch_total_tokens $max_batch_total_tokens"

    start_model_server $max_batch_total_tokens

    pushd ..
        ./benchmark_throughput.py \
            --port $PORT \
            --backend HfTextGenerationInference \
            --random_prompt_lens_mean ${random_prompt_lens_mean} \
            --random_prompt_lens_range 0 \
            --random_prompt_count 1000 \
            --gen_random_prompts \
            --variable_response_lens_mean ${response_len} \
            --variable_response_lens_range ${response_range} \
            --variable_response_lens_distribution ${distribution} \
            --allow_variable_generation_length \
            --model_path $model_name_dir \
            --max_num_threads $bs \
            --results_filename ${results_home}/tgi_gaudi_i${random_prompt_lens_mean}_o${response_len}_or${response_range}_bs${bs}.log
            
    popd

    kill_model_server
done
